{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c5bbdd"
      },
      "source": [
        "# üß† AI Agents Bootcamp: Running DeepSeek with CrewAI and Ollama\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vipbasil/aibootcamp/blob/main/Day%20I/crew_ollama_deepseek.ipynb?raw=true)\n",
        "\n",
        "This notebook is part of the AI Agents Bootcamp (23‚Äì27 June 2025) ‚Äî it shows how to:\n",
        "- Set up the `Ollama` environment in Colab\n",
        "- Run `DeepSeek` models locally for use in CrewAI/MAS pipelines\n",
        "- Prepare agents that use locally-hosted LLMs with memory and tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19eeeee4"
      },
      "source": [
        "## ‚öôÔ∏è Step 1: Environment Setup\n",
        "This installs and runs the `ollama` backend and exposes the service via localtunnel tunnel. Make sure to:\n",
        "- Restart the runtime if needed\n",
        "- Use the ngrok alternative (if Cloudflare is blocked or throttled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssm5RJKID82Z"
      },
      "outputs": [],
      "source": [
        "%pip install ollama\n",
        "%pip install colab-xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RimdapHHa7b9"
      },
      "source": [
        "## üõ†Ô∏è System Info Tools (Optional)\n",
        "\n",
        "Installs utilities (`pciutils`, `lshw`) to inspect hardware specs ‚Äî useful for checking GPU/CPU availability in Colab or local runtime.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBW3P8YuECqz"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install pciutils lshw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsSHIndzbDOB"
      },
      "source": [
        "## üì¶ Ollama Installation\n",
        "\n",
        "Downloads and installs Ollama via the official shell script ‚Äî run this once per environment setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwFE191pEGaI"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5285c8c4"
      },
      "source": [
        "## üîß Step 2: Programmatic Model Management and Server Initialization\n",
        "\n",
        "In this section, we:\n",
        "- Import the required libraries for managing subprocesses, HTTP requests, and multithreading\n",
        "- Start the Ollama server programmatically using a background thread\n",
        "- Pull the required models (`deepseek-r1:7b`, `llama3`) using `ollama pull`\n",
        "- Optionally include fallback to a smaller model (`deepseek-r1:1.5b`)\n",
        "- Confirm the list of available models and test that the local Ollama server is running at `localhost:11434`\n",
        "\n",
        "üìå **Why it matters**: This sets up your local model infrastructure for agent interaction. You'll later reference `localhost:11434` in your agent definitions to connect to these models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqoaeOpaEZZ8"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import threading\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRLABB5HZZo7"
      },
      "source": [
        "##  Launching the Ollama Server in Background\n",
        "\n",
        "Before using any model, we need to start the **Ollama inference server**, which listens by default on `localhost:11434`.\n",
        "\n",
        "This snippet:\n",
        "- Defines a Python function `run_ollama()` that launches `ollama serve`\n",
        "- Starts it in a **background thread**, so the notebook remains interactive\n",
        "- Allows the server to stay active without blocking further cells\n",
        "\n",
        "üõ†Ô∏è **Note**: You only need to run this once per session. If you restart your Colab, re-run this cell before using any models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f370227"
      },
      "outputs": [],
      "source": [
        "# Start the Ollama server\n",
        "def run_ollama():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "thread = threading.Thread(target=run_ollama)\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSnmd7ryZqNM"
      },
      "source": [
        "## üì• Pulling Models\n",
        "\n",
        "We download pre-trained models from the Ollama registry:\n",
        "- `deepseek-r1:7b` ‚Äì reasoning & code\n",
        "- `llama3` ‚Äì general-purpose assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6723baed"
      },
      "outputs": [],
      "source": [
        "# Download the deepseek-r1:7b distilled model\n",
        "!ollama pull deepseek-r1:7b\n",
        "!ollama pull llama3\n",
        "# If this doesn't work, you can uncomment the below code to download a smaller model- deepseek-r1:1.5b\n",
        "# !ollama pull deepseek-r1:1.5b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eZ8XjjMZ3ip"
      },
      "source": [
        "## ü™∂ Pulling Lightweight SLMs\n",
        "\n",
        "These small models are ideal for fast local agents and low-resource environments:\n",
        "- `phi3:mini`, `tinyllama` ‚Äì ultra-small general models\n",
        "- `gemma:2b` ‚Äì Google's compact chat model\n",
        "- `deepseek-r1:1.5b` ‚Äì distilled reasoning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbNtHtlRHlWG"
      },
      "outputs": [],
      "source": [
        "!ollama pull phi3:mini\n",
        "!ollama pull tinyllama\n",
        "!ollama pull gemma:2b\n",
        "!ollama pull deepseek-r1:1.5b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHS7WUwaMNk"
      },
      "source": [
        "## üîå Test Ollama Server\n",
        "\n",
        "Sends a test request to verify the Ollama server is running on `localhost:11434`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aae1987"
      },
      "outputs": [],
      "source": [
        "!curl http://127.0.0.1:11434"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5JaeB4YaAlx"
      },
      "source": [
        "## üìÑ Check Installed Models\n",
        "\n",
        "Lists all models currently downloaded and available in your local Ollama environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9809d934"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504891b1"
      },
      "source": [
        "# üß† Starting the CrewAI Section\n",
        "\n",
        "##Now we define agents using CrewAI, connected to our locally running Ollama models.  \n",
        "##This enables multi-agent workflows powered by lightweight, self-hosted LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f26b8d0b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üë®‚Äçü¶Ø Run this cell to hide all warnings (optional)\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# To avoid the restart session warning in Colab, exclude the PIL and\n",
        "# pydevd_plugins packages from being imported. This is fine because\n",
        "# we didn't execute the code in the kernel session afterward.\n",
        "\n",
        "# import sys\n",
        "# sys.modules.pop('PIL', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ca86609",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title ‚¨áÔ∏è Install project dependencies by running this cell\n",
        "%pip install git+https://github.com/joaomdmoura/crewAI.git --quiet\n",
        "%pip install crewai_tools langchain_openai langchain_groq langchain_anthropic langchain_community cohere --quiet\n",
        "print(\"---\")\n",
        "%pip show crewAI crewai_tools langchain_openai langchain_groq langchain_anthropic langchain_community cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f36bef0"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577d53c4"
      },
      "source": [
        "## üß© Step 3: CrewAI Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88866d05"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from textwrap import dedent\n",
        "from langchain_ollama import ChatOllama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb30a521"
      },
      "source": [
        "## Define Agents\n",
        "In CrewAI, agents are autonomous entities designed to perform specific roles and achieve particular goals. Each agent uses a language model (LLM) and may have specialized tools to help execute tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c416f4fe"
      },
      "outputs": [],
      "source": [
        "# @title üïµüèª Define your agents\n",
        "\n",
        "from crewai import Agent\n",
        "from textwrap import dedent\n",
        "from langchain_ollama import ChatOllama\n",
        "# Define LLM (OpenAI used here; replace as needed)\n",
        "from crewai import LLM\n",
        "\n",
        "llm = LLM(model=\"ollama/tinyllama:latest\", base_url=\"http://127.0.0.1:11434\")\n",
        "\n",
        "\n",
        "\n",
        "# Agent 1: Candidate Profiler\n",
        "agent_1 = Agent(\n",
        "    role=dedent(\"\"\"Candidate Profiler\"\"\"),\n",
        "    goal=dedent(\"\"\"Create a clear and structured professional portrait of the candidate based on the input data.\"\"\"),\n",
        "    backstory=dedent(\"\"\"You're a career analyst who knows how to form clear and concise professional profiles from brief descriptions.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Agent 2: Candidate Matcher\n",
        "agent_2 = Agent(\n",
        "    role=dedent(\"\"\"Candidate Matcher\"\"\"),\n",
        "    goal=dedent(\"\"\"Find suitable jobs from a list based on candidate profile including stack, budget and experience.\"\"\"),\n",
        "    backstory=dedent(\"\"\"You are a job matching expert. Your task is to match the candidate with the most suitable position from the available list.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Agent 3: Offer Writer\n",
        "agent_3 = Agent(\n",
        "    role=dedent(\"\"\"Offer Writer\"\"\"),\n",
        "    goal=dedent(\"\"\"Compose a polite and persuasive offer letter to the candidate based on the selected vacancy.\"\"\"),\n",
        "    backstory=dedent(\"\"\"You're an HR manager who is great at formulating letters, highlighting benefits and sounding professional but friendly.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Agent 4: Email Sender\n",
        "agent_4 = Agent(\n",
        "    role=dedent(\"\"\"Email Sender\"\"\"),\n",
        "    goal=dedent(\"\"\"Send an offer to the candidate via email (virtually).\"\"\"),\n",
        "    backstory=dedent(\"\"\"You're an email bot that receives the email and address and is responsible for delivering the email with a send confirmation.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "candidate_db = [\n",
        "    {\n",
        "        \"id\": \"c-101\",\n",
        "        \"full_name\": \"Alex Ivanov\",\n",
        "        \"email\": \"alex@devmail.com\",\n",
        "        \"role\": \"Full-Stack Developer\",\n",
        "        \"experience_years\": 5,\n",
        "        \"budget\": 4000,\n",
        "        \"stack\": [\"React\", \"Node.js\", \"PostgreSQL\", \"Docker\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"c-202\",\n",
        "        \"full_name\": \"Maria Petrova\",\n",
        "        \"email\": \"maria@devmail.com\",\n",
        "        \"role\": \"Backend Developer\",\n",
        "        \"experience_years\": 4,\n",
        "        \"budget\": 3500,\n",
        "        \"stack\": [\"Node.js\", \"MongoDB\", \"Docker\", \"AWS\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"c-303\",\n",
        "        \"full_name\": \"Oleg Smirnov\",\n",
        "        \"email\": \"oleg.smirnov@devmail.com\",\n",
        "        \"role\": \"DevOps Engineer\",\n",
        "        \"experience_years\": 6,\n",
        "        \"budget\": 4500,\n",
        "        \"stack\": [\"AWS\", \"Docker\", \"Kubernetes\", \"Terraform\", \"Ansible\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"c-404\",\n",
        "        \"full_name\": \"Elena Kozlova\",\n",
        "        \"email\": \"elena.kozlova@devmail.com\",\n",
        "        \"role\": \"Frontend Developer\",\n",
        "        \"experience_years\": 3,\n",
        "        \"budget\": 3200,\n",
        "        \"stack\": [\"Vue.js\", \"TypeScript\", \"TailwindCSS\", \"Vite\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"c-505\",\n",
        "        \"full_name\": \"Dmitry Lebedev\",\n",
        "        \"email\": \"dmitry.lebedev@devmail.com\",\n",
        "        \"role\": \"Data Engineer\",\n",
        "        \"experience_years\": 5,\n",
        "        \"budget\": 4800,\n",
        "        \"stack\": [\"Python\", \"Spark\", \"Airflow\", \"Kafka\", \"Redshift\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"c-606\",\n",
        "        \"full_name\": \"Anna Baranov\",\n",
        "        \"email\": \"anna.baranov@devmail.com\",\n",
        "        \"role\": \"QA Automation Engineer\",\n",
        "        \"experience_years\": 4,\n",
        "        \"budget\": 3300,\n",
        "        \"stack\": [\"Java\", \"Selenium\", \"Cypress\", \"JUnit\", \"Docker\"]\n",
        "    }\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6272d6"
      },
      "source": [
        "## Define Tasks\n",
        "Tasks in CrewAI are specific assignments given to agents, detailing the actions they need to perform to achieve a particular goal. Tasks can have dependencies and context, and can be executed asynchronously to ensure an efficient workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8249b567"
      },
      "outputs": [],
      "source": [
        "# @title üìù Define your tasks\n",
        "from crewai import Task\n",
        "from textwrap import dedent\n",
        "\n",
        "print(\"üëã –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ Recruitment-Crew\")\n",
        "role_inp      = input(\"üßë‚Äçüíª –†–æ–ª—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞?\\n>> \")\n",
        "exp_inp       = input(\"üìà –û–ø—ã—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–ª–µ—Ç)?\\n>> \")\n",
        "budget_inp    = input(\"üí∞ –ñ–µ–ª–∞–µ–º–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞?\\n>> \")\n",
        "stack_inp_raw = input(\"üõ† –°—Ç–µ–∫ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)?\\n>> \")\n",
        "\n",
        "candidate_input = {\n",
        "    \"role\": role_inp.strip(),\n",
        "    \"experience_years\": int(exp_inp),\n",
        "    \"budget\": int(budget_inp),\n",
        "    \"stack\": [s.strip() for s in stack_inp_raw.split(\",\")]\n",
        "}\n",
        "\n",
        "print(f\"‚úîÔ∏è Input received:\\nrole: {role_inp}\\nexperience_years: {exp_inp}\\nbudget: {budget_inp}\\nstack: {stack_inp_raw}\")\n",
        "\n",
        "# Task 1: Candidate Profiler\n",
        "task_profile = Task(\n",
        "    description=dedent(f\"\"\"\n",
        "        Using the input {candidate_input},\n",
        "        create a JSON profile of the candidate with the fields:\n",
        "        id (tmp-id), role, experience_years, budget, stack (list).\n",
        "    \"\"\"),\n",
        "    expected_output=\"JSON object candidate_profile\",\n",
        "    agent=agent_1,\n",
        "    output_key=\"candidate_profile\"\n",
        ")\n",
        "\n",
        "# Task 2: Candidate Matcher\n",
        "task_match = Task(\n",
        "    description=dedent(\"\"\"\n",
        "        Based on candidate_profile, find\n",
        "        in candidate_db the candidate that matches as much as possible:\n",
        "        - role match; - match or stack overlap;\n",
        "        - candidate's budget ‚â§ required;\n",
        "        - experience ‚â• required.\n",
        "        Return JSON matched_candidate (fields are the same as in the database)\n",
        "        + match_reason field (string).\n",
        "    \"\"\"),\n",
        "    expected_output=\"JSON matched_candidate\",\n",
        "    agent=agent_2,\n",
        "    context=[task_profile],\n",
        "    input_data={\"candidate_db\": candidate_db},\n",
        "    output_key=\"matched_candidate\"\n",
        ")\n",
        "\n",
        "# Task 3: Offer Writer\n",
        "task_offer = Task(\n",
        "    description=dedent(\"\"\"\n",
        "        Write an e-mail to the actual addressee matched_candidate.\n",
        "        Markdown format, 200-250 words.\n",
        "        Structure:\n",
        "          - greeting by name;\n",
        "          - a brief description of the position and the reasons for the match;\n",
        "          - terms: stack, salary, work format;\n",
        "          - call-to-action (answer, schedule a call).\n",
        "    \"\"\"),\n",
        "    expected_output=\"String offer_email\",\n",
        "    agent=agent_3,\n",
        "    context=[task_profile, task_match],\n",
        "    output_key=\"offer_email\"\n",
        ")\n",
        "\n",
        "# Task 4: Email Sender\n",
        "task_send = Task(\n",
        "    description=dedent(\"\"\"\n",
        "        Simulate sending the offer_email to email matched_candidate.\n",
        "        Return the line: 'Offer sent to {full_name} <{email}> | status: success'.\n",
        "    \"\"\"),\n",
        "    expected_output=\"String send_status\",\n",
        "    agent=agent_4,\n",
        "    context=[task_offer, task_match],\n",
        "    output_key=\"send_status\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bb55322"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c658581"
      },
      "outputs": [],
      "source": [
        "from crewai import Process  # –µ—Å–ª–∏ –µ—â—ë –Ω–µ—Ç\n",
        "\n",
        "def main():\n",
        "    crew = Crew(\n",
        "        agents=[agent_1, agent_2, agent_3, agent_4],\n",
        "        tasks=[task_profile, task_match, task_offer, task_send],\n",
        "        process=Process.sequential,\n",
        "        planning_llm=llm,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    crew.kickoff()  # —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Task-–∞—Ö\n",
        "\n",
        "    # 1Ô∏è‚É£ –ø—Ä–æ—Ñ–∏–ª—å\n",
        "    profile_raw = task_profile.output.raw\n",
        "\n",
        "    # 2Ô∏è‚É£ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç\n",
        "    match_raw = task_match.output.raw\n",
        "\n",
        "    # 3Ô∏è‚É£ –ø–∏—Å—å–º–æ-–æ—Ñ—Ñ–µ—Ä\n",
        "    offer_md = task_offer.output.raw\n",
        "\n",
        "    # 4Ô∏è‚É£ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–∫–∏\n",
        "    send_line = task_send.output.raw\n",
        "\n",
        "    print(\"\\nüë§ –ü–æ–¥—Ö–æ–¥—è—â–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç:\\n\", match_raw)\n",
        "    print(\"\\nüì§ –°—Ç–∞—Ç—É—Å –æ—Ç–ø—Ä–∞–≤–∫–∏:\\n\", send_line)\n",
        "\n",
        "    # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —á—Ç–æ-—Ç–æ –≤–µ—Ä–Ω—É—Ç—å –∏–∑ main():\n",
        "    return {\"candidate\": match_raw, \"status\": send_line}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    result = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "889206a0"
      },
      "outputs": [],
      "source": [
        "# @title üñ•Ô∏è Display the results of your crew as markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "markdown_text = final_status.raw  # Adjust this based on the actual attribute\n",
        "\n",
        "# Display the markdown content\n",
        "display(Markdown(markdown_text))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}